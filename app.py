"""
Streamlit web application for analyzing word frequencies in the Standard Works.

This application allows users to:
- Search for specific words or phrases.
- Optionally use regular expressions for searching.
- Optionally use lemmatization to find base forms of words.
- View frequency results as raw counts and instances per 10,000 words.
- Analyze results aggregated across all standard works, by individual standard work,
  or by individual books within each standard work.
- See a bar chart visualizing the instances per 10,000 words, ranked.

The app loads pre-processed word count data from a JSON file generated by
`data_processor.py`.
"""
import streamlit as st
import json
import pandas as pd
import re

# Attempt to import NLTK for lemmatizing search term on app side
try:
    import nltk
    from nltk.stem import WordNetLemmatizer
    # Ensure wordnet is available for the app's lemmatizer instance
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        # This is a fallback, data_processor.py should handle primary download.
        # If still not found, lemmatization in app might not be accurate.
        print("App: NLTK 'wordnet' resource not found. Lemmatization of search term might be affected.")
        # nltk.download('wordnet', quiet=True) # Avoid interactive download in app run
    
    app_lemmatizer = WordNetLemmatizer()
    APP_NLTK_AVAILABLE = True
except ImportError:
    print("App: NLTK library not found. Lemmatization of search term will not be performed.")
    APP_NLTK_AVAILABLE = False
    app_lemmatizer = None

# --- Page Configuration (must be the first Streamlit command) ---
st.set_page_config(
    layout="wide",
    page_title="Standard Works Word Frequency Analyzer",
    page_icon=":books:" # Optional: Add a page icon
)

# --- Constants ---
DATA_FILE = "data/processed_word_counts.json"
GRANULARITY_ALL_COMBINED = "All Standard Works Combined"
GRANULARITY_BY_SW = "By Standard Work"
GRANULARITY_BY_BOOK = "By Book (within each Standard Work)"

# --- Data Loading Function ---
@st.cache_data # Cache the data loading for performance
def load_data(file_path: str):
    """
    Loads the processed word count data from the specified JSON file.

    Args:
        file_path: The path to the JSON data file.

    Returns:
        A dictionary containing the scripture data, or None if loading fails.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # Check if lemmatized data likely exists (basic check)
        data["_has_lemmatized_data"] = False
        if "standard_works" in data:
            first_sw = next(iter(data["standard_works"].values()), None)
            if first_sw and "books" in first_sw:
                first_book = next(iter(first_sw["books"].values()), None)
                if first_book and "lemmatized_word_counts" in first_book:
                    data["_has_lemmatized_data"] = True
        if not data["_has_lemmatized_data"]:
            st.warning("Lemmatized word counts not found in the processed data. Lemmatization search will use raw word counts.")
        return data
    except FileNotFoundError:
        st.error(f"Error: Data file not found at {file_path}. Please run data_processor.py first.")
        return None
    except json.JSONDecodeError:
        st.error(f"Error: Could not decode JSON from {file_path}. The file might be corrupted or empty.")
        return None

# --- Search Logic Function ---
def get_search_results(search_term: str, data: dict, granularity: str, use_regex: bool, use_lemmatization: bool) -> pd.DataFrame:
    """
    Processes the loaded scripture data to find search term frequencies based on
    the selected granularity and search mode (exact or regex).

    Args:
        search_term: The word or regex pattern to search for.
        data: The loaded scripture data (dictionary from `load_data`).
        granularity: The level at which to aggregate results.
        use_regex: Boolean indicating whether to treat search_term as a regex pattern.
        use_lemmatization: Boolean indicating whether to use lemmatization.

    Returns:
        A Pandas DataFrame containing the search results.
    """
    results = []
    original_search_term = search_term # Keep for display
    search_input = search_term.strip()
    
    word_counts_field = "word_counts"
    processed_search_input = search_input.lower() # Default for exact match

    if use_lemmatization and data.get("_has_lemmatized_data") and APP_NLTK_AVAILABLE and app_lemmatizer:
        word_counts_field = "lemmatized_word_counts"
        # Lemmatize the search term itself if not using regex
        if not use_regex:
            # Simple lemmatization of the (potentially multi-word) search term.
            # For multi-word phrases, this might not be ideal without more sophisticated phrase handling.
            # Assuming single word or user knows how lemmatized phrases might look.
            lemmatized_terms = [app_lemmatizer.lemmatize(token.lower()) for token in search_input.split()]
            processed_search_input = " ".join(lemmatized_terms)
        else: # Regex with lemmatization: pattern applied to lemmatized words
            processed_search_input = search_input # Regex pattern used as is
    elif use_regex:
        processed_search_input = search_input # Regex pattern used as is
    
    if not processed_search_input:
        return pd.DataFrame()

    compiled_regex = None
    if use_regex:
        try:
            compiled_regex = re.compile(processed_search_input, re.IGNORECASE)
        except re.error as e:
            st.error(f"Invalid Regular Expression: {e}")
            return pd.DataFrame()

    def get_count_for_word_dict(word_dict: dict, term_to_match: str, is_regex: bool, regex_pattern) -> int:
        count = 0
        if is_regex and regex_pattern:
            for word, num in word_dict.items(): # word_dict here are the keys from lemmatized_word_counts or word_counts
                if regex_pattern.fullmatch(word):
                    count += num
        elif not is_regex:
            count = word_dict.get(term_to_match, 0)
        return count

    if granularity == GRANULARITY_ALL_COMBINED:
        overall_raw_count = 0
        overall_total_words = data.get("total_words_overall", 0)

        for sw_data in data.get("standard_works", {}).values():
            for book_data in sw_data.get("books", {}).values():
                current_word_counts = book_data.get(word_counts_field, {}) # Use selected field
                overall_raw_count += get_count_for_word_dict(
                    current_word_counts, 
                    processed_search_input, 
                    use_regex, 
                    compiled_regex
                )
        
        normalized_count_float = (overall_raw_count / overall_total_words) * 10000 if overall_total_words > 0 else 0.0
        results.append({
            "Scope": "All Standard Works",
            "Search Term/Pattern": original_search_term,
            "Raw Count": overall_raw_count,
            "Per 10,000 Words": normalized_count_float,
            "Total Words in Scope": f"{overall_total_words:,}"
        })
    
    elif granularity == GRANULARITY_BY_SW:
        for sw_name, sw_data in data.get("standard_works", {}).items():
            sw_raw_count = 0
            sw_total_words = sw_data.get("total_words_in_standard_work", 0)
            for book_data in sw_data.get("books", {}).values():
                current_word_counts = book_data.get(word_counts_field, {})
                sw_raw_count += get_count_for_word_dict(
                    current_word_counts, 
                    processed_search_input, 
                    use_regex, 
                    compiled_regex
                )
            
            normalized_count_float = (sw_raw_count / sw_total_words) * 10000 if sw_total_words > 0 else 0.0
            results.append({
                "Standard Work": sw_name,
                "Search Term/Pattern": original_search_term,
                "Raw Count": sw_raw_count,
                "Per 10,000 Words": normalized_count_float,
                "Total Words in Standard Work": f"{sw_total_words:,}"
            })

    elif granularity == GRANULARITY_BY_BOOK:
        for sw_name, sw_data in data.get("standard_works", {}).items():
            for book_name, book_data in sw_data.get("books", {}).items():
                current_word_counts = book_data.get(word_counts_field, {})
                book_raw_count = get_count_for_word_dict(
                    current_word_counts, 
                    processed_search_input, 
                    use_regex, 
                    compiled_regex
                )
                book_total_words = book_data.get("total_words", 0)
                normalized_count_float = (book_raw_count / book_total_words) * 10000 if book_total_words > 0 else 0.0
                results.append({
                    "Standard Work": sw_name,
                    "Book": book_name,
                    "Search Term/Pattern": original_search_term,
                    "Raw Count": book_raw_count,
                    "Per 10,000 Words": normalized_count_float,
                    "Total Words in Book": f"{book_total_words:,}"
                })
    
    return pd.DataFrame(results)


# --- Main Application Logic ---
scripture_data = load_data(DATA_FILE)
st.title("Standard Works Word Frequency Analyzer")

if scripture_data:
    st.sidebar.header("Search Options")
    search_term_input = st.sidebar.text_input(
        "Enter word or Regex pattern:", 
        value="faith",
        help="Enter a term. Behavior depends on search mode selections below."
    )
    use_regex_search = st.sidebar.checkbox(
        "Enable Regex Search", value=False, 
        help="Treat search term as a (case-insensitive) Regular Expression."
    )
    if use_regex_search:
        st.sidebar.info("Regex search lets you use patterns to match words. For example, `faith.*` matches 'faith', 'faithful', 'faithfulness', etc. Regex is case-insensitive by default.")

    use_lemmatization_search = st.sidebar.checkbox(
        "Enable Lemmatization", value=False, 
        help="Search for base forms of words (e.g., 'run' finds 'running'). Term itself also lemmatized. Relies on pre-processed data."
    )
    if use_lemmatization_search:
        st.sidebar.info("Lemmatization reduces words to their base or dictionary form. For example, 'running', 'ran', and 'runs' are all reduced to 'run'. This helps you find all forms of a word in your search.")
    if use_lemmatization_search and not scripture_data.get("_has_lemmatized_data", False):
        st.sidebar.warning("Lemmatization data not found in source. Search will use raw words.")
    if use_lemmatization_search and not APP_NLTK_AVAILABLE:
        st.sidebar.warning("NLTK not available in app. Lemmatization of search term may not be accurate.")

    granularity_options = [GRANULARITY_ALL_COMBINED, GRANULARITY_BY_SW, GRANULARITY_BY_BOOK]
    selected_granularity = st.sidebar.radio(
        "Select Granularity:", options=granularity_options, index=1,
        help="Choose the level of detail for the search results."
    )
    
    st.sidebar.markdown("---")
    # Build the search mode description dynamically
    active_modes = []
    if use_lemmatization_search:
        active_modes.append("Lemmatized")
    if use_regex_search:
        active_modes.append("Regex")
    
    if not active_modes: # Neither Lemmatization nor Regex is selected
        search_mode_text = "Exact (case-insensitive) search."
    elif len(active_modes) == 1: # Only one of Lemmatization or Regex is selected
        search_mode_text = f"{active_modes[0]} search."
        if active_modes[0] == "Regex":
            search_mode_text += " (Case-insensitive by default)"
    else: # Both Lemmatization and Regex are selected
        search_mode_text = "Lemmatized and Regex search. (Regex is case-insensitive by default)"

    st.sidebar.markdown(f"**Search Mode:** {search_mode_text}")

    st.markdown("--- ")
    st.header("Search Results")

    if not search_term_input.strip():
        st.warning("Please enter a search term/pattern in the sidebar.")
    else:
        st.write(f"Searching for: **'{search_term_input.strip()}'**")
        st.write(f"Mode: {search_mode_text}")
        st.write(f"Granularity: **{selected_granularity}**")
        
        df_results = get_search_results(
            search_term_input, scripture_data, 
            selected_granularity, use_regex_search, use_lemmatization_search
        )

        if not df_results.empty:
            df_display = df_results.copy()
            if "Per 10,000 Words" in df_display.columns:
                 df_display["Per 10,000 Words"] = df_display["Per 10,000 Words"].apply(lambda x: f"{x:.2f}")
            st.dataframe(df_display, use_container_width=True)

            st.subheader("Instances per 10,000 Words (Ranked)")
            df_chart = df_results.copy()
            y_axis_label = "Scope"
            if selected_granularity == GRANULARITY_BY_SW:
                y_axis_label = "Standard Work"
            elif selected_granularity == GRANULARITY_BY_BOOK:
                y_axis_label = "Book"
            
            if y_axis_label in df_chart.columns and "Per 10,000 Words" in df_chart.columns and not df_chart.empty:
                df_chart_sorted = df_chart.sort_values(by="Per 10,000 Words", ascending=False)
                st.bar_chart(df_chart_sorted.set_index(y_axis_label)["Per 10,000 Words"])
            else:
                if not df_chart.empty:
                    st.info("Chart cannot be displayed for the current granularity or data configuration.")
        else:
            if search_term_input.strip():
                 # Check if it was a regex error that resulted in empty df
                if not (use_regex_search and re.fullmatch(search_term_input.strip(), "") is None and not compiled_regex):
                    st.info(f"The term/pattern '{search_term_input.strip()}' was not found or resulted in no data for the selected options.")
else:
    st.error("Critical Error: Failed to load scripture data.")

# Informational footer (optional)
# st.markdown("---")
# st.caption("Powered by Streamlit and Python.")

# To run the app: streamlit run app.py 